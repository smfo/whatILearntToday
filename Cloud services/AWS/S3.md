
# Hosting files with S3

The AWS service for storing files in the region you specify.

Object: file and metadata
Bucket: stores objects

## Objects
An object can be of any filetype.

All object has a object key. This consists of the objects' file name and the folder name `folderName/fileName.fileType`.

## Buckets
Stores objects and creates a url namespace for these. The url depends on the region and bucket name, therefor the bucketname need to be uniquie
all through S3.

By default buckets are only available for the AWS account that created it. Other users can be given permissions to access the bucket. This can also be applied to
everyone. ex. for a web recourse everyone should be able to view the bucket content, but not neccessarily modify them.\
Another option is to use S3 or IAM policies.

The contenct of a bucket is available from S3.

## Creating a bucket
Through S3 create a bucket with the deciered permissions.

Permissions can be changed/specified later on, see policyGenerator.

## Uploading objects
Objects can be uploaded using the console, CLI or SDK.

Console is good for a small number of files. The CLI is good for bulk uploads and the SDK is for dynamic in code uploading.

### CLI
* Open the CLI in the local project folder
* Type the command to execute

aws s3 cp pathToFolder/FileFromCurrentLocation s3://bucketName/folderName --recursive --exclude "filetypes to exclude"\
s3: the service we want to use, cp: copy, from the given path on the computer to the given path in s3. Recursive: copy the entire folder except what is speccified in exclude.

The user who is registered in the AWS Cli (key/secret key) has to have permission to upload files to this bucket.

### S3 console
Navigate to the location in the bucket where the files are to be uploaded. Click the upload button and select the files to be uploaded.\
It is possible to set permissions and properties for each upload batch.

### SDK
Upload files that are dynamically created and saved in the application.

First create a call to the S3 bucket.
```javascript
// this works because we have configured the CLI with our AWS credentials
const AWS = require('aws-sdk');
const s3 = new AWS.S3()

module.exports.save = (name, data) => {
    return new Promise((resolve, reject) => {
        const params = {
            Bucket: 'pizza-lovers-synne', // bucket name
            Key: `pizzas/${name}.png`, //path for the new object
            Body: Buffer.from(data, 'base64'),
            ContentEncoding: 'base64',
            ContentType: 'image/png'
        }

        s3.putObject(params, (err, data) => {
            err ? reject : resolve(`//pizza-lovers-synne.s3.eu-north-1.amazonaws.com/${parmas.Key}`) //route path + object key
        })
    })
}
```

Then import call this function to save data to s3.
```javascript
const s3Store = require('./imageStoreS3')

function save (name, base64String) {
  const imageData = base64String.split('data:image/png;base64,')[1]
  return s3Store.save(name, imageData) //call the save function and put to S3
}

module.exports = {
  save
}
```

**CORS**\
Need to enable CORS for S3 to post data using SDK because the buckets are not in the same domain as the local application.

In S3 select the bucket - permissions - CORS configuration.

```HTML
This allows all origins to get data from the bucket.
<CORSConfiguration>
<CORSRule>
    <AllowedOrigin>*</AllowedOrigin>
    <AllowedMethod>GET</AllowedMethod>
    <MaxAgeSeconds>3000</MaxAgeSeconds>
</CORSRule>
</CORSConfiguration>
```

## Connection to S3 using local code
Get the route path the files share. Select a file in the bucket and copy the object URL, drop the specific folder path and image name as well as the protocoll name (https:, http:).
The path should start with `//`.

When refering to files in S3 from the code, use the route path ( and then the specific foler/file path ) instead of a path to a local folder/file. If the files are already referenced locally,
this to the S3 route.

## Connecting to EC2
When using localhost it is the AWS user credentials, on an EC2 application IAM roles are used to set credentials. 
This needs to be done to give the EC2 instance access to S3 buckets and can be set in the launch configurations of the auto scaling group.

EC2 needs to have the code that interacts with S3 instead of a local store. Upload files in the EC2 instance and create a new AIM using these files if needed.\
Create a new launch configuration using the new AMI and assign the role with access to S3, see administration. To access S3 the instances need to go onto the public internett, so in configure details - advanced details - IP address type
chose Assign a public IP address to every instance. Otherwise configure just as before.

In Auto scaling groups - details - launch configuration use a configuration that has the deciered role with access to S3 and a public IP.